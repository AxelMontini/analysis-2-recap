
\begin{definition}{3.3.5}
    $f: X \mapsto \R $ has a partial derivative with respect to the $i$-th variable if the function
    \[ g(t) = f(x_{0,1}, ..., x_{0,i-1}, t, x_{0,i+1}, ..., x_{0,n}) \]
    is differentiable for all $x_0 \in X$ on the set $I = \left\{ t \in \R \mid (x_{0,1}, ..., x_{0,i-1}, t, x_{0,i+1}, ..., x_{0,n}) \in X \right\}$.

    Its derivative $g'(x_{0,i})$ is denoted
    \[ \frac{\partial f}{\partial x_i}(x_0),\ \partial_{x_i}(x_0),\ \partial_i (x_0) \]
\end{definition}

\begin{proposition}{3.3.7}
    $x \subset \R^n$ open, $f,g$ functions from $X$ to $\R^m$. Let $1 \le 1 \le n$.
    \begin{enumerate}
        \item if $f, g$ have partial derivatives of $i$-th coordinate on $X$, then $f + g$ also does.
              $\partial_{x_i}(f + g) = \partial_{x_i}(f) + \partial_{x_i}(g)$
        \item if the previous is true and $m = 1$, then $fg$ also does and $\partial_{x_i}(f g) = \partial_{x_i}(f) \cdot g + f \cdot \partial_{x_i}(g)$
        \item If the previous is true and $g(x) \ne 0$ for all $x \in X$, then $f / g$ has a partial derivative
              $\partial_{x_i}(f/g) = \left (\partial_{x_i}(f)g - f \partial_{x_i}(g) \right)/g^2$
    \end{enumerate}
\end{proposition}

\begin{definition}{3.3.9}
    $f: X \mapsto \R^m$ has partial derivatives on $X$. Write $f(x) = (f_1(x), f_2(x), ..., f_m(x))$.

    For any $x \in X$, the \textbf{Jacobi Matrix} ($m$ rows, $n$ columns) of $f$ at $x$ is defined as
    \[ J_f(x) = (\partial_{x_i} f_i(x))_{\stackrel{1 \le i \le m}{1 \le j \le n}} \]
\end{definition}

\begin{definition}{3.3.11}
    \begin{enumerate}
        \item If all partial derivatives of $f: X \mapsto \R$ exist at $x_0 \in X$, then the column vector
              \[ \nabla f(x_0) = \begin{pmatrix}
                      \partial_{x_1} f(x_0) \\
                      ...                   \\
                      \partial_{x_n} f(x_0)
                  \end{pmatrix} \]
              is called the \textbf{gradient} of $f$ at $x_0$.
        \item Let $f = (f_1, f_2, ..., f_n): X \mapsto \R^n$ and all partial derivatives of all coordinates $f_i$ of $f$ exist at $x_0 \in X$. Then
              \[ Tr(J_f(x_0)) = \sum_{i=1}^{n} \partial_{x_i} f_i(x_0) \]
              is the \textit{trace} of the Jacobi Matrix and is called the \textbf{divergence} of $f$ at $x_0$, also $div(f)(x_0)$
    \end{enumerate}

\end{definition}

\begin{definition}{3.4.2}
    Let $X \subset \R^n$ be open and $f: X \mapsto \R^m$ be a function.
    Let $u$ be a linear map $\R^n \mapsto \R^m$ and $x_0 \in X$. We say that $f$ is \textit{differentiable} at
    $x_0$ with differential $u$ if
    \[ \lim_{\stackrel{x \to x_0}{x \ne 0}} \frac{1}{\Vert x - x_0 \Vert}(f(x) - f(x_0) - u(x - x_0)) = 0 \]
    We then denote $df(x_0) = u$. If it is differentiable at every $x_0 \in X$, then it is differentiable on $X$.

    Then, close to $x_0$, we can approximate $f(x)$ by $g(x) = f(x_0) + u(x - x_0)$
\end{definition}

\begin{proposition}{3.4.4}
    Let $X \subset \R^n$ be open and $f: X \mapsto \R^m$ be a function differentiable on $X$. Then
    \begin{enumerate}
        \item $f$ is \textbf{continuous} on $X$.
        \item $f$ admits partial derivatives on $X$ with respect to each variable.
        \item Assume that $m = 1$. Let $x_0 \in X$ and $u(x_1, ..., x_n) = a_1 x_1 + ... + a_n x_n$ be the differential of $f$ at $x_0$.
              We then have $\partial_{x_i} f(x_0) = a_i$ for $1 \le i \le n$
    \end{enumerate}
\end{proposition}

\begin{proposition}{3.4.6}
    $X \subset R^n$ open, $f: x \mapsto R^m$, $g: X \mapsto R^m$ differentiable functions on $X$.
    \begin{enumerate}
        \item $f + g$ is differentiable on $X$ with differential $d(f + g) = df + df$.
        \item If $m = 1$, then $fg$ is differentiable. If we also have $g(x) \ne 0$ for all
              $x \in X$, then $f/g$ is differentiable.
    \end{enumerate}
\end{proposition}

\begin{proposition}{3.4.7}
    If $f$ has all partial derivatives on $X$ and they are all continuous on $X$, then $f$ is differentiable on $X$.
    The matrix of the differential $df(x_0)$ is the Jacobi Matrix of $f$ at $x_0$.
\end{proposition}

\begin{proposition}{3.4.9 (Chain Rule)}
    Let $X \subset \R^n$ and $Y \subset \R^m$ be open.
    Let $f: X \mapsto Y$ and $g: Y \mapsto R^p$ be differentiable functions.
    Then $g \circ f: X \mapsto \R^p$ is differentiable on $X$, and for any $x \in X$, its differential is given by
    \[ d(g \circ f)(x_0) = dg(f(x_0)) \circ df(x_0) \]
    The Jacobi Matrix satisfies
    \[ J_{g \circ f}(x_0) = J_g(f(x_0)) J_f(x_0)\ \mbox{(matrix product)}\]
\end{proposition}


\begin{definition}
    Let $X \subset \R^n$ be open and $f: X \mapsto \R^m$ differentiable.
    Let $x_0 \in X$ and $u = df(x_0)$ be the differential of $f$ at $x_0$. The graph of the affine linear approximation
    \[ g(x) = f(x_0) + u(x - x_0) \]
    from $\R^n$ to $\R^m$, or in other words the set
    \[ \left\{ (x,y) \in \R^n \times \R^m \mid y = f(x_0) + u(x - x_0) \right\} \]
    is called the \textbf{tangent space} at $x_0$ to the graph of $f$.
\end{definition}

\begin{definition}{3.4.13 (Directional Derivative)}
    Let $X \subset \R^n$ be open, $f: X \mapsto \R^m$ a function.

    Let $v \in \R^n$ be a non-zero vector and $x_0 \in X$.

    We say that $f$ has \textbf{directional derivative} $w \in \R^m$ in the direction $v$
    if the function $g$ defined on the set $I$ has a derivative at $t = 0$ and this is equal to $w$.
    \[ g(t) = f(x_0 + tv), \quad I = \left\{ t \in \R \mid x_0 + tv \in R \right\} \]

    Other words: limit is equal to $w$
    \[ \lim_{\stackrel{t \to 0}{t \ne 0}} \frac{f(x_0 + tv) - f(x_0)}{t} \]
\end{definition}

\begin{proposition}{3.4.15}
    $X$ open, $f$ differentiable. Then for any $x_0 \in X$ and non-zero $v$, the function has
    a directional derivative at $x_0$ in the direction $v$, equal to $df(x_0)(v)$
\end{proposition}

\begin{definition}{3.5.1}
    Let $X \subset \R^n$ be open and $f: X \mapsto \R^m$.
    We say that $f$ is of class:
    \begin{itemize}
        \item $C^1$ is differentiable on $X$ and all partial derivatives are continuous.
        \item $C^k$ if differentiable on $X$ and all partial derivatives $\partial_{x_i}f: X \mapsto \R^m$ are of class $C^{k-1}$.
        \item $C^\infty$ if $f \in C^k(X;\R^m)$ for all $k \ge 1$
    \end{itemize}
    Set of functions of class $C^k$ from $X$ to $\R^m$ is denoted $C^k(X; \R^m)$
\end{definition}

\begin{proposition}{3.5.4 (Mixed derivatives commute)}
    $X \subset \R^n$ open and $f: X \mapsto \R^m$ of class $C^k$. Then the partial derivatives
    of order $k$ are independent of the order in which the partial derivatives are taken: for any variables
    $x_1, x_2, ..., x_n$ we have
    \[ \partial_{x_1, x_2, ..., x_n}f = \partial_{x_2, x_1, ..., x_n}f = ...   \ \mbox{(all combinations)}\]
\end{proposition}

\begin{definition}{3.5.9 (Hessian)}
    Let $X \subset \R^n$ be open and $f: X \mapsto \R$ a $C^2$ function.
    For $x \in X$, the \textbf{Hessian matrix} of $f$ at $x$ is the \textbf{symmetrix square} matrix
    \[ \mathit{H}_f(x) = \mathrm{Hess}_f(x) = (\partial_{x_i, x_j}f)_{1 \le i, j \le n} \]
\end{definition}

\begin{definition}{(Second partial derivative test)}
    Using the Hessian matrix and its determinant, one can find critical points of a function and
    (most of the times) determine whether it's a min/max or a saddle point.

    Define $D(a, b) = \det(H_f(a,b))$.

\begin{itemize}
    \item \textbf{Two dimensions}: to find all critical points one looks for solutions of $f_x(a,b) = f_y(a,b) = 0$
    and then characterizes them as follows:
    \begin{enumerate}
        \item If $D(a,b) > 0$ and $f_{XX}(a,b) > 0$ then $(a,b)$ is a local minimum of $f$.
        \item If $D(a,b) > 0$ and $f_{XX}(a,b) < 0$ then $(a,b)$ is a local maximum.
        \item If $D(a,b) < 0$ then $(a,b)$ is a saddle point.
        \item If $D(a,b) = 0$ then the test is inconclusive.
    \end{enumerate}
    
    \item \textbf{Multiple variables}: one must look at the eigenvalues of the Hessian matrix at $(a,b)$.
    \begin{enumerate}
        \item If all eigenvalues are positive, then it's a local minimum.
        \item If all eigenvalues are negative, then it's a maximum.
        \item If there's both positive and negative eigenvalues, then it's a saddle point.
        \item Otherwise, the test is inconclusive.
    \end{enumerate}
\end{itemize}
    
    
\end{definition}

\begin{example}{(Change of variable)}
    Idea: create $h$, which is $f$ on a different coordinate system.

    Open set $U \subset \R^n$ containing the new variables $(y_1, ..., y_n)$ and a change of variable $g: U \mapsto X$
    that expresses $(x_1, ..., x_n)$ in terms of $(y_1, ..., y_n)$.

    Consider $x_1 = g_1(y_0, ..., y_n),\quad x_n = g_n(y_1, ..., y_n)$

    Composite $h = f \circ g: U \mapsto \R$ is the function $f$ expressed in terms of the new variables $y$.

    Polar coordinates: Map $g: U \mapsto \R^2,\ g(r, \theta) = (r \cos \theta, r \sin \theta)$.
    Replace $f$ by $h$: $h(r, \theta) = f(r \cos \theta, r \sin \theta)$.
    \[ J_g(r, \theta) = \begin{pmatrix}
            \cos \theta & - r \sin \theta \\
            \sin \theta & r \cos \theta
        \end{pmatrix} \]
\end{example}

\subsection{Taylor Polynomials}
\begin{definition}{3.7.1 (Taylor polynomials)}
    Let $k \ge 1$ be an integer, $f: X \mapsto \R$ a function of class $C^k$ on $X$, and fix $x_0 \in X$. The
    $k$-th Taylor polynomial of $f$ at point $x_0$ is the poly in $n$ variables of degree $\le k$ given by
    \begin{align*}
        T_k f(y;x_0) = & f(x_0) + \sum_{i=1}^n \frac{\partial f}{\partial x_i}(x_0)y_i + ...                                                                          \\
                       & + \sum_{m_1 + ... + m_n = k} \frac{1}{m_1!...m_n!} \frac{\partial^k f}{\partial x_1^{m_1} ... \partial x_n^{m_n}} (x_0)y_1^{m_1}...y_n^{m_n}
    \end{align*}
    where the last sum ranges over the tuples of $n$ positive integers such that the sum is $k$.

    Case $n=1$ (one variable):
    \[ T_k f(y; x_0) = f(x_0) + f'(x_0)y + \frac{f''(x_0)}{2}y^2 + ... + \frac{f^{(k)}(x_0)}{k!}y^k \]
\end{definition}

\begin{proposition}{3.7.3 (Taylor Approximation)}
    $k \ge 1$, $X \subset \R^n$ open, $f: X \mapsto \R$ a $C^k$ function. For $x_0$ in $X$, we define $E_k f(x;x_0)$ by
    \[ f(x) = T_k f(x - x_0;x_0) + E_k f(x;x_0) \]
    then we have
    \[ \lim_{\stackrel{x \to x_0}{x \ne x_0}} \frac{e_k f(x;x_0)}{\Vert x - x_0 \Vert^k} = 0 \]
\end{proposition}


\subsection{Critical Points}
\begin{proposition}{3.8.1}
    Let $X \subset \R^n$ be open and $f: X \mapsto \R$ be differentiable.
    If $x_0 \in X$ is a local maximum or a local minimum, then we have (equivalent) for $1 \le i \le n$:
    \[ df(x_0) = 0,\ \nabla f(x_0) = 0,\ \frac{\partial f}{\partial x_i}(x_0) = 0 \]
\end{proposition}

\begin{definition}{3.8.2 (Critical Point)}
    Let $X$ be open and $f$ be differentiable.
    A point $x_0$ is called a \textbf{critical point} of $f$ if $\nabla f(x_0) = 0$.
\end{definition}

\begin{definition}{3.8.6 (Non-degenerate critical point)}
    $f$ of class $C^2$. A critical point $x_0$ is \textbf{non-degenerate} if the Hessian matrix has non-zero determinant.
\end{definition}

\begin{corollary}{3.8.7}
    $X$ open and $f: X \mapsto \R$ of class $C^2$. Let $x_0$ be a non-degenerate critical point of $f$. Let $p, q$ be the number of positive and negative eigenvalues of $Hess_f (x_0)$
    \begin{enumerate}
        \item if $p = n$, equivalently if $q = 0$, the function $f$ has a local minimum at $x_0$.
        \item if $q = n$, equivalently if $p = 0$, $f$ has a local maximum at $x_0$.
        \item Otherwise, the function $f$ does not have a local extremum at $x_0$, equivalently it has a saddle point at $x_0$.
    \end{enumerate}
\end{corollary}

\subsection{Lagrange multipliers}

\begin{proposition}{3.9.2 (Lagrange Multiplier)}
    Let $X \subset \R^n$ be open and $f,g: X \mapsto \R$ be class $C^1$.
    If $x_0 \in X$ is a local extremum of $f$ restricted to the set
    $Y = \left\{ x \in X \mid g(x) = 0 \right\}$ ($\nabla f(x_0)$ can be non-zero!),
    then either $\nabla g(x_0) = 0$ or there exist $\lambda$ such that
    \[ \begin{cases}
            \nabla f(x_0) = \lambda \nabla g(x_0) \\
            g(x_0) = 0
        \end{cases} \]

    In other words, $(x_0, \lambda)$ is a critical point of $h(x, \lambda) = f(x) - \lambda g(x)$.

    Value $\lambda$ is the Lagrange Multiplier at $x_0$.
\end{proposition}

\subsection{The inverse and implicit functions theorems}

\begin{definition}{3.10.1 (Change of variable)}

\end{definition}

\begin{theorem}{3.10.2 (Inverse function theorem)}
    $X \subset \R^n$ open and $f: X \mapsto \R^n$ differentiable.
    If the jacobian matrix of $f$ at $x_0 \in X$ is invertible ($det(J_f(x_0)) \ne 0$) then $f$ is a change of variable around $x_0$.

    Moreover, $J_g(f(x_0)) = J_f(x_0)^{-1}$.

    In addition, if $f$ is of class $C^k$, then $g$ is also of class $C^k$.
\end{theorem}

\begin{theorem}{3.10.4 (Implicit function theorem)}
    Let $X \subset \R^{n + 1}$ be open, $g: X \mapsto \R$ be of class $C^k$ with $k \ge 1$. Let $(x_0, y_0) \in \R^n \times \R$
    such that $g(x_0, y_0) = 0$.

    Assume that $\partial_y g(x_0, y_0) \ne 0$.

    Then there exists an open set $U \subset \R^n$ containing $x_0$, an open interval $I \subset \R$ containing $y_0$, and a function
    $f: u \mapsto \R$ of class $C^k$ such that the system of equations
    \[ \begin{cases}
            g(x,y) = 0 \\
            x \in U,\ y \in I
        \end{cases} \]
    is equivalent with $y = f(x)$. In particular, $f(x_0) = y_0$. Moreover, the gradient of $f$ at $x_0$ is
    \[ \nabla f(x_0) = - \frac{1}{(\partial_y g)(x_0, y_0)} \nabla_x g(x_0, y_0) \]
    where $\nabla_x g = (\partial_{x_1} g, ..., \partial_{x_n} g)$
\end{theorem}
